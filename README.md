# ğŸ“ Student Dropout Prediction using Machine Learning

This project aims to predict whether a student is at risk of dropping out based on factors like attendance, grades, and participation using a classification model (Logistic Regression).

---

## ğŸ“Œ Problem Statement

Predict whether a student will **drop out** based on their **attendance**, **grades**, and **participation**. This is a binary classification problem (0 = Not At Risk, 1 = At Risk).

---

## ğŸ§  Approach

We follow a traditional machine learning pipeline:

- Data Preprocessing
- Feature Engineering
- Train-test split
- Logistic Regression
- Evaluation with Accuracy, Precision, Recall, F1 Score
- Visualization using Confusion Matrix

---

## ğŸ—ƒï¸ Dataset

The dataset contains the following columns:

- `attendance`  
- `grades`  
- `participation`  
- `dropout_risk` (Target Variable: yes/no)

---

## ğŸ§¹ Data Preprocessing

1. Checked for missing values.
2. Applied one-hot encoding to convert `dropout_risk` to numeric.
3. Used `StandardScaler` to scale numeric features.
4. Split data into training (80%) and testing (20%) sets.

---

## ğŸ§ª Model Implementation

We used **Logistic Regression** for classification. It is simple, fast, and effective for binary classification problems like this.

---

## ğŸ“Š Evaluation Metrics

The model was evaluated using:

- âœ… **Accuracy**
- ğŸ“Œ **Precision**
- ğŸ¯ **Recall**
- ğŸ“ **F1 Score**
- ğŸ“‰ **Confusion Matrix** (heatmap)

---

## ğŸ–¼ï¸ Screenshots




> ğŸ” *These screenshots help visualize the overall classification performance and problem scope.*

---

## ğŸ“Œ Results and Analysis

- The Logistic Regression model gave **reasonable accuracy** on the test set.
- The **confusion matrix** showed a good balance between false positives and false negatives.
- **Precision** and **recall** scores indicate the model can reliably flag potential dropouts.

---

## âœ… Conclusion

Logistic Regression was effective in predicting student dropout risk. While this baseline model works well, future improvements may include:

- Using Random Forest or Gradient Boosting
- Addressing class imbalance
- Adding more student features (e.g., socio-economic background, feedback, etc.)

---

## ğŸ™Œ Credits

- ğŸ“‚ Dataset from local drive (`student_dropout.csv`)
- ğŸ Libraries: Pandas, Scikit-learn, Seaborn, Matplotlib
- ğŸ‘¨â€ğŸ’» Developed in Google Colab

